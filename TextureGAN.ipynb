{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe51fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb68774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from argparser import parse_arguments\n",
    "from PIL import Image\n",
    "from main import get_transforms\n",
    "from dataloader import imfol\n",
    "from dataloader.imfol import ImageFolder, make_dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from utils.visualize import vis_patch, vis_image\n",
    "from models import texturegan,discriminator\n",
    "from train import gen_input, rand_between, gen_input_rand\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from utils import transforms as custom_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58501ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '--display_port 7770 --load 0 --load_D -1 --load_epoch 105 --gpu 2 --model texturegan --feature_weight 1e2 --pixel_weight_ab 1e3 --global_pixel_weight_l 1e3 --local_pixel_weight_l 0 --style_weight 0 --discriminator_weight 1e3 --discriminator_local_weight 1e6  --learning_rate 1e-4 --learning_rate_D 1e-4 --batch_size 36 --save_every 50 --num_epoch 100000 --save_dir /hdd/2017IS013/researchGroup03/Pytorch_ARGAN/data/train_txt --load_dir /hdd/2017IS013/researchGroup03/Pytorch_ARGAN/data/train_txt --data_path /hdd/2017IS013/researchGroup03/Pytorch_ARGAN/data/ --learning_rate_D_local  1e-4 --local_texture_size 50 --patch_size_min 20 --patch_size_max 40 --num_input_texture_patch 1 --visualize_every 5 --num_local_texture_patch 1'\n",
    "args = parse_arguments(command.split())\n",
    "    \n",
    "args.batch_size = 1\n",
    "args.image_size =152\n",
    "args.resize_max = 256\n",
    "args.resize_min = 256\n",
    "    \n",
    "transform = get_transforms(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968831b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defaulty used\n",
    "\n",
    "def convertToStandard(input_img, texture):\n",
    "#convert input sketch to canny \n",
    "#path should be changed here\n",
    "#     input_img = cv2.imread(\"data/78.jpeg\") #path to cloth folder\n",
    "    #plt.imshow(input_img)\n",
    "    \n",
    "     \n",
    "    txt_path = \"C:/Users/ASUS/Desktop/OurGAN/data/texture.jpg\"\n",
    "    cv2.imwrite(txt_path,texture)\n",
    "    \n",
    "    #plt.imshow(texture)\n",
    "# args.data_path = '/hdd/2017IS013/researchGroup03/Pytorch_ARGAN/data/' #change to your data path\n",
    "    img_path = \"data/blank.jpg\"\n",
    "    \n",
    "    gray = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray,100,200)\n",
    "    ret,th2 = cv2.threshold(edges,100,255,cv2.THRESH_BINARY_INV)\n",
    "    sketch_image = \"data/sketchmyfrock6.jpg\"  #path to save sketches images\n",
    "    cv2.imwrite(sketch_image, th2)\n",
    "    \n",
    "    skg_path = sketch_image\n",
    "    img = cv2.imread(skg_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, np.mean(gray), 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)[1]\n",
    "    edges = cv2.dilate(cv2.Canny(thresh,0,255), None)\n",
    "    cnt = sorted(cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2], key=cv2.contourArea)[-1]\n",
    "    \n",
    "    masked = []\n",
    "    segmented = []\n",
    "\n",
    "    mask = np.zeros((256,256), np.uint8)\n",
    "    masked.append(cv2.drawContours(mask, [cnt], -1, 255, -1))\n",
    "\n",
    "    dst = cv2.bitwise_and(img, img, mask=mask)\n",
    "    segmented = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    filename = \"data/seg.jpg\"\n",
    "    cv2.imwrite(filename, mask)\n",
    "    \n",
    "    seg_path = filename\n",
    "    eroded_seg_path = filename\n",
    "   \n",
    "   # texture = cv2.imread(texture)\n",
    "\n",
    "    \n",
    "    img = pil_loader(img_path)\n",
    "    skg = pil_loader(skg_path)\n",
    "    seg = pil_loader(seg_path)\n",
    "    txt = pil_loader(txt_path)\n",
    "    eroded_seg = pil_loader(eroded_seg_path)\n",
    "    img, skg, seg, eroded_seg, txt = transform([img, skg, seg, eroded_seg, txt])\n",
    "    img = img.unsqueeze(0)\n",
    "    skg = skg.unsqueeze(0)\n",
    "    txt = txt.unsqueeze(0)\n",
    "    seg = seg.unsqueeze(0)\n",
    "    eroded_seg = eroded_seg.unsqueeze(0)\n",
    "    data = [img, skg, seg, eroded_seg, txt]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skg_path = sketch_image\n",
    "# img = cv2.imread(skg_path)\n",
    "# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# thresh = cv2.threshold(gray, np.mean(gray), 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)[1]\n",
    "# edges = cv2.dilate(cv2.Canny(thresh,0,255), None)\n",
    "# cnt = sorted(cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2], key=cv2.contourArea)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5356ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(model, save_path):\n",
    "        \n",
    "    model_state = torch.load(save_path)\n",
    "    \n",
    "    if \"state_dict\" in model_state:\n",
    "        model.load_state_dict(model_state[\"state_dict\"])\n",
    "    else:\n",
    "        model.load_state_dict(model_state)\n",
    "\n",
    "        model_state = {\n",
    "            'state_dict': model.cpu().state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'iteration': iteration,\n",
    "            'model': args.model,\n",
    "            'color_space': args.color_space,\n",
    "            'batch_size': args.batch_size,\n",
    "            'dataset': dataset,\n",
    "            'image_size': args.image_size\n",
    "        }\n",
    "    \n",
    "    model.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c30dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(val_loader,xcenter,ycenter,patch_size,num_patch):\n",
    "    img, skg, seg, eroded_seg, txt = val_loader\n",
    "    img = custom_transforms.normalize_lab(img)\n",
    "    skg = custom_transforms.normalize_lab(skg)\n",
    "    txt = custom_transforms.normalize_lab(txt)\n",
    "    seg = custom_transforms.normalize_seg(seg)\n",
    "    eroded_seg = custom_transforms.normalize_seg(eroded_seg)\n",
    "\n",
    "    bs, w, h = seg.size()\n",
    "\n",
    "    seg = seg.view(bs, 1, w, h)\n",
    "    seg = torch.cat((seg, seg, seg), 1)\n",
    "\n",
    "    eroded_seg = eroded_seg.view(bs, 1, w, h)\n",
    "    eroded_seg = torch.cat((eroded_seg, eroded_seg, eroded_seg), 1)\n",
    "\n",
    "    temp = torch.ones(seg.size()) * (1 - seg).float()\n",
    "    temp[:, 1, :, :] = 0  # torch.ones(seg[:,1,:,:].size())*(1-seg[:,1,:,:]).float()\n",
    "    temp[:, 2, :, :] = 0  # torch.ones(seg[:,2,:,:].size())*(1-seg[:,2,:,:]).float()\n",
    "\n",
    "    txt = txt.float() * seg.float() + temp\n",
    "\n",
    "    patchsize = args.local_texture_size\n",
    "    batch_size = bs\n",
    "    if xcenter < 0 or ycenter < 0:\n",
    "        inp, texture_loc = gen_input_rand(txt, skg, eroded_seg[:, 0, :, :] * 100,\n",
    "                                              patch_size, patch_size,\n",
    "                                              num_patch)\n",
    "    else:\n",
    "        inp, texture_loc = gen_input_exact(txt, skg, eroded_seg[:, 0, :, :] * 100,xcenter,ycenter,patch_size,1)\n",
    "        \n",
    "    return inp,texture_loc \n",
    "\n",
    "def get_inputv(inp):\n",
    "    input_stack = torch.FloatTensor().cpu()\n",
    "    input_stack.resize_as_(inp.float()).copy_(inp)\n",
    "    inputv = Variable(input_stack)\n",
    "    return inputv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(sketch, texture):\n",
    "    \n",
    "    color_space = 'lab'\n",
    "\n",
    "    #data = valLoader.__iter__().__next__()\n",
    "#     input_img1 = cv2.imread(\"data/78.jpeg\")\n",
    "#     input_img2 = cv2.imread(\"data/sea.jpeg\")\n",
    "    \n",
    "    data = convertToStandard(sketch, texture)\n",
    "    img, skg, seg, eroded_seg, txt = data\n",
    "\n",
    "    img = custom_transforms.normalize_lab(img)\n",
    "    skg = custom_transforms.normalize_lab(skg)\n",
    "    txt = custom_transforms.normalize_lab(txt)\n",
    "    seg = custom_transforms.normalize_seg(seg)\n",
    "    eroded_seg = custom_transforms.normalize_seg(eroded_seg)\n",
    "    inp,texture_loc = get_input(data,-1,-1,30,1)\n",
    "\n",
    "    seg = seg!=0\n",
    "    \n",
    "    model_location = 'trained_model/final_cloth_finetune.pth'\n",
    "\n",
    "    netG = texturegan.TextureGAN(5, 3, 32)\n",
    "    load_network(netG, model_location)\n",
    "    netG.eval()\n",
    "    \n",
    "    model = netG\n",
    "\n",
    "    inpv = get_inputv(inp.cpu())\n",
    "    output = model(inpv.cpu())\n",
    "\n",
    "    out_img = vis_image(custom_transforms.denormalize_lab(output.data.double().cpu()),\n",
    "                                        color_space)\n",
    "    inp_img = vis_patch(custom_transforms.denormalize_lab(txt.cpu()),\n",
    "                                custom_transforms.denormalize_lab(skg.cpu()),\n",
    "                                texture_loc,\n",
    "                                color_space)\n",
    "    tar_img = vis_image(custom_transforms.denormalize_lab(img.cpu()),\n",
    "                            color_space)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(np.transpose(inp_img[0],(1, 2, 0)))\n",
    "#     plt.axis('off')\n",
    "    #plt.figure()  \n",
    "#     plt.figure()\n",
    "#     plt.imshow(np.transpose(out_img[0],(1, 2, 0)))\n",
    "#     plt.axis('off')\n",
    "    return np.transpose(out_img[0],(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output(sketch, texture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch = gr.inputs.Image()\n",
    "texture = gr.inputs.Image()\n",
    "print(texture)\n",
    "\n",
    "iface = gr.Interface(fn=output, inputs=[sketch, texture], outputs=\"image\")\n",
    "iface.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d263295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
