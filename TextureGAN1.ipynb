{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe51fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcb68774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from argparser import parse_arguments\n",
    "from PIL import Image\n",
    "from main import get_transforms\n",
    "from dataloader import imfol\n",
    "from dataloader.imfol import ImageFolder, make_dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from utils.visualize import vis_patch, vis_image\n",
    "from models import texturegan,discriminator\n",
    "from train import gen_input, rand_between, gen_input_rand\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from utils import transforms as custom_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf78978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: Flask-Cors>=3.0.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (3.0.10)\n",
      "Requirement already satisfied: pillow in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (8.4.0)\n",
      "Requirement already satisfied: pycryptodome in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (3.12.0)\n",
      "Requirement already satisfied: pydub in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (3.4.3)\n",
      "Requirement already satisfied: Flask>=1.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (1.1.2)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: Flask-Login in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: flask-cachebuster in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: markdown2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (2.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (1.3.4)\n",
      "Requirement already satisfied: analytics-python in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (1.4.0)\n",
      "Requirement already satisfied: paramiko in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (2.7.2)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gradio) (2.26.0)\n",
      "Requirement already satisfied: click>=5.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from Flask>=1.1.1->gradio) (8.0.3)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from Flask>=1.1.1->gradio) (2.11.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from Flask>=1.1.1->gradio) (2.0.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from Flask>=1.1.1->gradio) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from click>=5.1->Flask>=1.1.1->gradio) (0.4.4)\n",
      "Requirement already satisfied: Six in c:\\users\\asus\\anaconda3\\lib\\site-packages (from Flask-Cors>=3.0.8->gradio) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->Flask>=1.1.1->gradio) (1.1.1)\n",
      "Requirement already satisfied: backoff==1.10.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from analytics-python->gradio) (1.10.0)\n",
      "Requirement already satisfied: python-dateutil>2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from analytics-python->gradio) (2.8.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from analytics-python->gradio) (1.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->gradio) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->gradio) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->gradio) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->gradio) (1.26.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->gradio) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->gradio) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->gradio) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->gradio) (2021.3)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from paramiko->gradio) (3.2.0)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from paramiko->gradio) (1.4.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from paramiko->gradio) (3.4.8)\n",
      "Requirement already satisfied: cffi>=1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from bcrypt>=3.1.3->paramiko->gradio) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio) (2.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58501ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf13bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '--display_port 7770 --load 0 --load_D -1 --load_epoch 105 --gpu 2 --model texturegan --feature_weight 1e2 --pixel_weight_ab 1e3 --global_pixel_weight_l 1e3 --local_pixel_weight_l 0 --style_weight 0 --discriminator_weight 1e3 --discriminator_local_weight 1e6  --learning_rate 1e-4 --learning_rate_D 1e-4 --batch_size 36 --save_every 50 --num_epoch 100000 --save_dir /hdd/2017IS013/researchGroup03/Pytorch_ARGAN/data/train_txt --load_dir /hdd/2017IS013/researchGroup03/Pytorch_ARGAN/data/train_txt --data_path /hdd/2017IS013/researchGroup03/Pytorch_ARGAN/data/ --learning_rate_D_local  1e-4 --local_texture_size 50 --patch_size_min 20 --patch_size_max 40 --num_input_texture_patch 1 --visualize_every 5 --num_local_texture_patch 1'\n",
    "args = parse_arguments(command.split())\n",
    "    \n",
    "args.batch_size = 1\n",
    "args.image_size =152\n",
    "args.resize_max = 256\n",
    "args.resize_min = 256\n",
    "args.data_path = '/hdd/2017IS013/researchGroup03/Pytorch_ARGAN/data/' #change to your data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da30015",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = get_transforms(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "968831b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defaulty used\n",
    "\n",
    "def convertToStandard(input_img, texture):\n",
    "#convert input sketch to canny \n",
    "#path should be changed here\n",
    "#     input_img = cv2.imread(\"data/78.jpeg\") #path to cloth folder\n",
    "    #plt.imshow(input_img)\n",
    "    \n",
    "    \n",
    "    \n",
    "# args.data_path = '/hdd/2017IS013/researchGroup03/Pytorch_ARGAN/data/' #change to your data path\n",
    "    #img_path = \"data/blank.jpg\"\n",
    "    img_path = \"C:/Users/ASUS/Desktop/OurGAN/data/blank.jpg\"\n",
    "    plt.imshow(img_path)\n",
    "    #path should be changed here\n",
    "    #input_img = cv2.imread(\"C:/Users/ASUS/Desktop/OurGAN/data/78.jpeg\") #path to cloth folder\n",
    "    \n",
    "    gray = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray,100,200)\n",
    "    ret,th2 = cv2.threshold(edges,100,255,cv2.THRESH_BINARY_INV)\n",
    "    sketch_image = \"C:/Users/ASUS/Desktop/OurGAN/data/sketchmyfrock6.jpg\"  #path to save sketches images\n",
    "    #sketch_image = \"data/sketchmyfrock6.jpg\"  #path to save sketches images\n",
    "    cv2.imwrite(sketch_image, th2)\n",
    "    \n",
    "    skg_path = sketch_image\n",
    "    img = cv2.imread(skg_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #plt.imshow(gray)\n",
    "    thresh = cv2.threshold(gray, np.mean(gray), 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)[1]\n",
    "    #plt.imshow(thresh)\n",
    "    edges = cv2.dilate(cv2.Canny(thresh,0,255), None)\n",
    "    #plt.imshow(edges)\n",
    "    cnt = sorted(cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2], key=cv2.contourArea)[-1]\n",
    "    \n",
    "    masked = []\n",
    "    segmented = []\n",
    "\n",
    "    mask = np.zeros((256,256), np.uint8)\n",
    "    masked.append(cv2.drawContours(mask, [cnt], -1, 255, -1))\n",
    "\n",
    "    dst = cv2.bitwise_and(img, img, mask=mask)\n",
    "    segmented = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\n",
    "    filename = \"C:/Users/ASUS/Desktop/OurGAN/data/seg.jpg\"\n",
    "    #filename = \"data/seg.jpg\"\n",
    "    cv2.imwrite(filename, mask)\n",
    "    \n",
    "    seg_path = filename\n",
    "    eroded_seg_path = filename\n",
    "    #Set the texture patch path here\n",
    "    txt_path = \"C:/Users/ASUS/Desktop/OurGAN/data/sea.jpeg\"\n",
    "    #txt_path = \"data/texture.jpg\"\n",
    "    \n",
    "    #cv2.imwrite(txt_path,mask)\n",
    "\n",
    "    img = pil_loader(img_path)\n",
    "    skg = pil_loader(skg_path)\n",
    "    seg = pil_loader(seg_path)\n",
    "    txt = pil_loader(txt_path)\n",
    "    eroded_seg = pil_loader(eroded_seg_path)\n",
    "    img, skg, seg, eroded_seg, txt = transform([img, skg, seg, eroded_seg, txt])\n",
    "    img = img.unsqueeze(0)\n",
    "    skg = skg.unsqueeze(0)\n",
    "    txt = txt.unsqueeze(0)\n",
    "    seg = seg.unsqueeze(0)\n",
    "    eroded_seg = eroded_seg.unsqueeze(0)\n",
    "    data = [img, skg, seg, eroded_seg, txt]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "942e6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skg_path = sketch_image\n",
    "# img = cv2.imread(skg_path)\n",
    "# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# thresh = cv2.threshold(gray, np.mean(gray), 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)[1]\n",
    "# edges = cv2.dilate(cv2.Canny(thresh,0,255), None)\n",
    "# cnt = sorted(cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2], key=cv2.contourArea)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd5356ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(model, save_path):\n",
    "        \n",
    "    model_state = torch.load(save_path)\n",
    "    \n",
    "    if \"state_dict\" in model_state:\n",
    "        model.load_state_dict(model_state[\"state_dict\"])\n",
    "    else:\n",
    "        model.load_state_dict(model_state)\n",
    "\n",
    "        model_state = {\n",
    "            'state_dict': model.cpu().state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'iteration': iteration,\n",
    "            'model': args.model,\n",
    "            'color_space': args.color_space,\n",
    "            'batch_size': args.batch_size,\n",
    "            'dataset': dataset,\n",
    "            'image_size': args.image_size\n",
    "        }\n",
    "    \n",
    "    model.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c30dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(val_loader,xcenter,ycenter,patch_size,num_patch):\n",
    "    img, skg, seg, eroded_seg, txt = val_loader\n",
    "    img = custom_transforms.normalize_lab(img)\n",
    "    skg = custom_transforms.normalize_lab(skg)\n",
    "    txt = custom_transforms.normalize_lab(txt)\n",
    "    seg = custom_transforms.normalize_seg(seg)\n",
    "    eroded_seg = custom_transforms.normalize_seg(eroded_seg)\n",
    "\n",
    "    bs, w, h = seg.size()\n",
    "\n",
    "    seg = seg.view(bs, 1, w, h)\n",
    "    seg = torch.cat((seg, seg, seg), 1)\n",
    "\n",
    "    eroded_seg = eroded_seg.view(bs, 1, w, h)\n",
    "    eroded_seg = torch.cat((eroded_seg, eroded_seg, eroded_seg), 1)\n",
    "\n",
    "    temp = torch.ones(seg.size()) * (1 - seg).float()\n",
    "    temp[:, 1, :, :] = 0  # torch.ones(seg[:,1,:,:].size())*(1-seg[:,1,:,:]).float()\n",
    "    temp[:, 2, :, :] = 0  # torch.ones(seg[:,2,:,:].size())*(1-seg[:,2,:,:]).float()\n",
    "\n",
    "    txt = txt.float() * seg.float() + temp\n",
    "\n",
    "    patchsize = args.local_texture_size\n",
    "    batch_size = bs\n",
    "    if xcenter < 0 or ycenter < 0:\n",
    "        inp, texture_loc = gen_input_rand(txt, skg, eroded_seg[:, 0, :, :] * 100,\n",
    "                                              patch_size, patch_size,\n",
    "                                              num_patch)\n",
    "    else:\n",
    "        inp, texture_loc = gen_input_exact(txt, skg, eroded_seg[:, 0, :, :] * 100,xcenter,ycenter,patch_size,1)\n",
    "        \n",
    "    return inp,texture_loc \n",
    "\n",
    "def get_inputv(inp):\n",
    "    input_stack = torch.FloatTensor().cpu()\n",
    "    input_stack.resize_as_(inp.float()).copy_(inp)\n",
    "    inputv = Variable(input_stack)\n",
    "    return inputv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77cb1d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(sketch, texture):\n",
    "    \n",
    "    color_space = 'lab'\n",
    "\n",
    "    #data = valLoader.__iter__().__next__()\n",
    "#     input_img1 = cv2.imread(\"data/78.jpeg\")\n",
    "#     input_img2 = cv2.imread(\"data/sea.jpeg\")\n",
    "    \n",
    "    data = convertToStandard(sketch, texture)\n",
    "    img, skg, seg, eroded_seg, txt = data\n",
    "\n",
    "    img = custom_transforms.normalize_lab(img)\n",
    "    skg = custom_transforms.normalize_lab(skg)\n",
    "    txt = custom_transforms.normalize_lab(txt)\n",
    "    seg = custom_transforms.normalize_seg(seg)\n",
    "    eroded_seg = custom_transforms.normalize_seg(eroded_seg)\n",
    "    inp,texture_loc = get_input(data,-1,-1,30,1)\n",
    "\n",
    "    seg = seg!=0\n",
    "    \n",
    "    #model_location = 'trained_model/final_cloth_finetune.pth'\n",
    "    model_location = 'C:/Users/ASUS/Desktop/OurGAN/trained_model/final_cloth_finetune.pth'\n",
    "\n",
    "\n",
    "    netG = texturegan.TextureGAN(5, 3, 32)\n",
    "    load_network(netG, model_location)\n",
    "    netG.eval()\n",
    "    \n",
    "    model = netG\n",
    "\n",
    "    inpv = get_inputv(inp.cpu())\n",
    "    output = model(inpv.cpu())\n",
    "\n",
    "    out_img = vis_image(custom_transforms.denormalize_lab(output.data.double().cpu()),\n",
    "                                        color_space)\n",
    "    inp_img = vis_patch(custom_transforms.denormalize_lab(txt.cpu()),\n",
    "                                custom_transforms.denormalize_lab(skg.cpu()),\n",
    "                                texture_loc,\n",
    "                                color_space)\n",
    "    tar_img = vis_image(custom_transforms.denormalize_lab(img.cpu()),\n",
    "                            color_space)\n",
    "\n",
    "    #plt.figure()\n",
    "#     plt.imshow(np.transpose(inp_img[0],(1, 2, 0)))\n",
    "#     plt.axis('off')\n",
    "    #plt.figure()  \n",
    "#     plt.figure()\n",
    "  #  plt.imshow(np.transpose(out_img[0],(1, 2, 0)))\n",
    "   # plt.axis('off')\n",
    "    return np.transpose(out_img[0],(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "448d91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output(sketch, texture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740e46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860/\n",
      "Running on public URL: https://38851.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"https://38851.gradio.app\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1c25a172220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\gradio\\networking.py\", line 237, in predict\n",
      "    prediction, durations = app.interface.process(raw_input)\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\gradio\\interface.py\", line 411, in process\n",
      "    predictions, durations = self.run_prediction(\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\gradio\\interface.py\", line 374, in run_prediction\n",
      "    prediction = predict_fn(*processed_input)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp/ipykernel_4224/3873811009.py\", line 9, in output\n",
      "    data = convertToStandard(sketch, texture)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp/ipykernel_4224/3769621804.py\", line 14, in convertToStandard\n",
      "    plt.imshow(img_path)\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\", line 2903, in imshow\n",
      "    __ret = gca().imshow(\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\", line 1361, in inner\n",
      "    return func(ax, *map(sanitize_sequence, args), **kwargs)\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\", line 5609, in imshow\n",
      "    im.set_data(X)\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\matplotlib\\image.py\", line 700, in set_data\n",
      "    raise TypeError(\"Image data of dtype {} cannot be converted to \"\n",
      "TypeError: Image data of dtype <U43 cannot be converted to float\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\gradio\\networking.py\", line 237, in predict\n",
      "    prediction, durations = app.interface.process(raw_input)\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\gradio\\interface.py\", line 411, in process\n",
      "    predictions, durations = self.run_prediction(\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\gradio\\interface.py\", line 374, in run_prediction\n",
      "    prediction = predict_fn(*processed_input)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp/ipykernel_4224/3873811009.py\", line 9, in output\n",
      "    data = convertToStandard(sketch, texture)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp/ipykernel_4224/3769621804.py\", line 14, in convertToStandard\n",
      "    plt.imshow(img_path)\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\", line 2903, in imshow\n",
      "    __ret = gca().imshow(\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\", line 1361, in inner\n",
      "    return func(ax, *map(sanitize_sequence, args), **kwargs)\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\", line 5609, in imshow\n",
      "    im.set_data(X)\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\matplotlib\\image.py\", line 700, in set_data\n",
      "    raise TypeError(\"Image data of dtype {} cannot be converted to \"\n",
      "TypeError: Image data of dtype <U43 cannot be converted to float\n"
     ]
    }
   ],
   "source": [
    "sketch = gr.inputs.Image()\n",
    "texture = gr.inputs.Image()\n",
    "\n",
    "iface = gr.Interface(fn=output, inputs=[sketch, texture], outputs=\"image\")\n",
    "iface.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d263295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf049dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
